{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jonathan Rossi**  \n",
    "*April 21, 2016*  \n",
    "Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Variable Linear Regression Algorithm #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of a single-variable linear regression algorithm. We use gradient descent to find the values\n",
    "of $\\theta_0$ and $\\theta_1$—the intercept and slope (x-coefficient), respectively—that minimize the sum of squared residuals of the\n",
    "regression line, which gives us a \"line of best fit.\" This code is based on concepts used in the following places:\n",
    "\n",
    "* https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
    "* http://www.bogotobogo.com/python/python_numpy_batch_gradient_descent_algorithm.php\n",
    "* https://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient Descent Primer\n",
    "* Implementation\n",
    "* Implementation Walkthrough\n",
    "* Example Dataset and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Primer ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of linear regression is to find a line that best fits a scatter plot of data points. A common definition of \"line of best fit\" is that the line minimizes the sum of the squared residuals (i.e., minimizes the sum of the distances between each point and the line). We define a \"cost function\" $J(\\theta_0,\\,\\theta_1)$ that equals exactly this sum:\n",
    "\n",
    "$$J(\\theta_0,\\,\\theta_1) \\,\\, = \\,\\, \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1x_i) - y_i)^2$$\n",
    "\n",
    "where $\\theta_0 + \\theta_1x_i$ is the equation for our regression line. $\\theta_0$ represents the intercept of the regression line and $\\theta_1$ represents the slope. Our goal is to find $\\theta_0$ and $\\theta_1$ such that $J(\\theta_0,\\,\\theta_1)$ is minimized. In practice, we will minimize the following:\n",
    "\n",
    "$$\\underset{\\theta_0,\\,\\theta_1}{\\text{minimize}}\\,\\frac{1}{2m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1x_i - y_i)^2$$\n",
    "\n",
    "We adjust the sum by the term $\\frac{1}{2m}$ by convention. Dividing by $m$ gives us the average squared residual, and the factor of $\\frac{1}{2}$ gives us a cleaner expression when we take the derivative later on for gradient descent (the multiple of $2$ from the exponent cancels with the $\\frac{1}{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to find $\\theta_0$ and $\\theta_1$ such that $J(\\theta_0,\\,\\theta_1)$ is minimized. In fact, with a linear regression, we can compute the optimal values directly. However, part of the objective of this exercise was to use gradient descent, and so that is what we will do.\n",
    "\n",
    "Gradient Descent is a widely used optimization algorithm. Applying it to our cost function, $J(\\theta_0,\\,\\theta_1)$, we are going to follow two steps:\n",
    "\n",
    "* Initialize $\\theta_0$ and $\\theta_1$ to be some values. There are smart ways to choose the values to initialize to but we will simply initialize to $0$ and that will serve our purposes.\n",
    "* Repeatedly update the values for $\\theta_0$ and $\\theta_1$ until our algorithm converges on a minimum for $J(\\theta_0,\\,\\theta_1)$.\n",
    "\n",
    "Our update step will look like this:\n",
    "\n",
    "$$\\theta_j \\, := \\,\\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\,\\theta_1)\\,\\,\\,\\,\\,\\,\\text{for $j$ = $0, 1$}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate of the algorithm, which we can change to fine tune how the algorithm converges. It is important to note that in order for the algorithm to work properly, we need to update $\\theta_0$ and $\\theta_1$ simultaneously. We can accomplish that by doing the following:\n",
    "\n",
    "$$temp_0 := \\,\\theta_0 - \\alpha\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\,\\theta_1)$$\n",
    "$$temp_1 := \\,\\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\,\\theta_1)$$\n",
    "\n",
    "and only then updating $\\theta_0$ and $\\theta_1$:\n",
    "\n",
    "$$\\theta_0 := temp_0$$\n",
    "$$\\theta_1 := temp_1$$\n",
    "\n",
    "The following video gives some intuition behind why adjusting $\\theta_0$ and $\\theta_1$ by their respective partial derivatives of the cost function leads us to a minimum: https://www.youtube.com/watch?v=Fn8qXpIcdnI. In short, the slope of the cost function (i.e., the derivative) with respect to $\\theta_j \\,$ (for $j$ = $0,1$) is negative if we are too far to the left of the cost function's minimum (i.e., $\\theta_j$ is too small). In this case, we subtract a negative, which makes $\\theta_j$ bigger. If we are too far to the right of the minimum, the derivative of the cost function with respect to $\\theta_j$ is positive. This time, we subtract a positive, which makes $\\theta_j$ smaller. In this way, we move closer and closer to the optimal value of $\\theta_j$. Furthermore, as we get closer to the minumum, the derivative of the cost function gets smaller, and so we adjust $\\theta_j$ by a smaller and smaller amount each time. This allows us to converge on an optimal value.\n",
    "\n",
    "Given our cost function $J(\\theta_0,\\,\\theta_1)$, we take the partial derivatives with respect to $\\theta_0$ and $\\theta_1$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1x_i - y_i)^2\\,\\,\\,\\,\\,\\,\\text{for $j$ = $0, 1$}$$\n",
    "\n",
    "which evaluate to:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1x_i - y_i)$$\n",
    "$$\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1x_i - y_i)\\,x_i$$\n",
    "\n",
    "So, our update step, using $J(\\theta_0,\\,\\theta_1)$ as our cost function, looks like:\n",
    "\n",
    "$$\\theta_0 \\, := \\,\\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1x_i - y_i)$$\n",
    "$$\\theta_1 \\, := \\,\\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1x_i - y_i)\\,x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Single-Variable Linear Regression model class.\n",
    "class SLRegression(object):\n",
    "    def __init__(self, learnrate = .01, tolerance = .000000001, max_iter = 10000):\n",
    "        # learnrate (float): the learning rate for the regression.\n",
    "        # tolerance (float): our threshold for defining \"convergence.\"\n",
    "        # max_iter (int): the maximum number of iterations we will allow.\n",
    "\n",
    "        self.learnrate = learnrate\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def change_learnrate(self, new_learnrate):\n",
    "        # new_learnrate (float): the new learning rate.\n",
    "        self.learnrate = new_learnrate\n",
    "\n",
    "    def change_tolerance(self, new_tolerance):\n",
    "        # new_tolerance (float): the new tolerance.\n",
    "        self.tolerance = new_tolerance\n",
    "\n",
    "    def change_max_iter(self, new_max_iter):\n",
    "        # new_max_iter (int): thenew maximum number of iterations.\n",
    "        self.max_iter = new_max_iter\n",
    "\n",
    "    # Define fit function.\n",
    "    def fit(self, data):\n",
    "        # data (array-like, shape = [m_observations, 2_columns]): the training data.\n",
    "\n",
    "        converged = False\n",
    "        m = data.shape[0]\n",
    "            # converged (bool): whether algorithm has converged.\n",
    "            # m (int): the number of samples.\n",
    "\n",
    "        # Initialize other class variables.\n",
    "        self.iter_ = 0\n",
    "        self.theta0_ = 0\n",
    "        self.theta1_ = 0\n",
    "\n",
    "        # Compute the \"cost\" function J.\n",
    "        J = (1.0/(2.0*m)) * sum([(self.theta0_ + self.theta1_*data[i][1] - data[i][0])**2 for i in range(m)])\n",
    "\n",
    "        # Recursively update theta0 and theta1.\n",
    "        while not converged:\n",
    "            self.iter_ += 1\n",
    "\n",
    "            # Calculate the partial derivatives of J with respect to theta0 and theta1.\n",
    "            pdtheta0 = (1.0/m) * sum([(self.theta0_ + self.theta1_*data[i][1] - data[i][0]) for i in range(m)])\n",
    "            pdtheta1 = (1.0/m) * sum([(self.theta0_ + self.theta1_*data[i][1] - data[i][0]) * data[i][1] for i in range(m)])\n",
    "\n",
    "            # Subtract the learnrate * partial derivative from theta0 and theta1.\n",
    "            temp0 = self.theta0_ - (self.learnrate * pdtheta0)\n",
    "            temp1 = self.theta1_ - (self.learnrate * pdtheta1)\n",
    "\n",
    "            # Update theta0 and theta1.\n",
    "            self.theta0_ = temp0\n",
    "            self.theta1_ = temp1\n",
    "\n",
    "            # Compute the updated cost function, given new theta0 and theta1.\n",
    "            new_J = (1.0/(2.0*m)) * sum([(self.theta0_ + self.theta1_*data[i][1] - data[i][0])**2 for i in range(m)])\n",
    "\n",
    "            # Test for convergence.\n",
    "            if abs(J - new_J) <= self.tolerance:\n",
    "                converged = True\n",
    "                print(('Model converged after %s iterations!') % (self.iter_))\n",
    "\n",
    "            # Set old cost equal to new cost.\n",
    "            J = new_J\n",
    "\n",
    "            # Test whether we have hit max_iter.\n",
    "            if self.iter_ == self.max_iter:\n",
    "                converged = True\n",
    "                print('Maximum iterations have been reached!')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # Define a \"point forecast\" function.\n",
    "    def point_forecast(self, x):\n",
    "        return self.theta0_ + self.theta1_ * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Walkthrough ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a detailed, step-by-step explanation of the Python implementation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class. ###\n",
    "\n",
    "* **Define the initializer.**\n",
    "    \n",
    "    * A note about `tolerance`: our \"best fit\" line is considered converged, when the difference between the cost function with the old values of `theta0` and `theta1` and updated cost function is less than or equal to the `tolerance`.\n",
    "    \n",
    "\n",
    "* **Define helper functions**, `change_learn_rate()`, `change_tolerance()`, and `change_max_iter()`, that allow us to easily change the initial parameters of the regression model.\n",
    "\n",
    "\n",
    "* **Define a 'fit' function** that uses gradient descent to fit a regression line to our data (i.e., finds optimal `theta0` and `theta1`).\n",
    "\n",
    "    * `data` has the shape `[m_observations, 2_columns]`, where the 2 columns are the dependent-variable values in the first column, and the independent-variable values in the second column.\n",
    "    \n",
    "    * Initialize local variables `converged` and `m`. We will continue to update `theta0` and `theta1` until `converged` is `True`, and `m` is the number of samples we pass to the function.\n",
    "        * `data.shape` returns `(num_rows, num_cols)`, so `data.shape[0] = num_rows` and `data.shape[1] = num_cols`.\n",
    "    * Initialize additional class variables, `self.iter_`, `self.theta0_`, and `self.theta1_`. By convention, we use `_` to denote any attributes of the SLRegression class that are not part of the class initialization.\n",
    "        * Initialize the number of iterations, `self.iter_`, to `0` and track it as we go.\n",
    "        * Initialize `self.theta0_` and `self.theta1_` to `0`. The values we use for initialization can affect how the algorithm converges, but initializing to `0` will serve our purposes.\n",
    "        \n",
    "    * Compute the \"cost\" function `J(theta0, theta1)`, which computes the sum of the squared residuals, given `theta0` and `theta1`. This is the sum we want to minimize in order to find optimal `theta1` and `theta0`. Note, the square brackets in the `sum` expression ensure that we return a `list` instead of a `generator` (either way, the result will be a single number).\n",
    "    * Recursively update `theta0` and `theta1` until `converged == True` or `self.iter_ == self.max_iter`.\n",
    "        * Increase `self.iter_`.\n",
    "        * Calculate the partial derivatives, `pdtheta0` and `pdtheta1`, of J with respect to `theta0` and `theta1`. These are the \"differences\" by which we will adjust our current `theta0` and `theta1`.\n",
    "        * Subtract the `learnrate` multiplied by the partial derivatives from `theta0` and `theta1`.\n",
    "        * Update `theta0` and `theta1`.\n",
    "        * Compute the updated cost function, given new `theta0` and `theta1`.\n",
    "        * Test for convergence.\n",
    "        * Set old cost equal to new cost.\n",
    "        * Test whether we have hit max_iter.\n",
    "        \n",
    "    \n",
    "    \n",
    "* **Define a 'point forecast' function.**\n",
    "    \n",
    "    * Given feature value `x`, `point_forecast()` returns the regression's predicted value for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Dataset and Results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converged after 8034 iterations!\n",
      "After 8034 iterations, the model converged on Theta0 = 0.0731944487608 and Theta1 = 0.870240770913.\n",
      "Scipy linear regression gives intercept: 0.0706191941281 and slope = 0.874682774148.\n",
      "As an example, our algorithm gives y = 0.830303919455, given x = .87.\n",
      "The true y-value for x = .87 is about .8368.\n"
     ]
    }
   ],
   "source": [
    "# Read in the data (temperature (column 1) versus sales (column 2)).\n",
    "data = np.squeeze(np.array(pd.read_csv('sales_normalized.csv')))\n",
    "\n",
    "# Create a regression model with the default learning rate, tolerance, and maximum number of iterations.\n",
    "slregression = SLRegression()\n",
    "\n",
    "# Call the fit function and pass in our data.\n",
    "slregression.fit(data)\n",
    "\n",
    "# Print out the results.\n",
    "print(('After %s iterations, the model converged on Theta0 = %s and Theta1 = %s.') % (slregression.iter_, slregression.theta0_, slregression.theta1_))\n",
    "\n",
    "# Compare our model to scipy linregress model.\n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(data[:,1], data[:,0])\n",
    "print(('Scipy linear regression gives intercept: %s and slope = %s.') % (intercept, slope))\n",
    "\n",
    "# Test the model with a point forecast.\n",
    "print(('As an example, our algorithm gives y = %s, given x = .87.') % (slregression.point_forecast(.87))) # Should be about .83.\n",
    "print('The true y-value for x = .87 is about .8368.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
